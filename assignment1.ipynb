{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPjLVoLDffjWHMrL+gHLo9y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gulrukhsorakhmadjanova/5dpro/blob/master/assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAPcoqlwN5L_"
      },
      "outputs": [],
      "source": [
        "#  Setup and Imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from skimage import data, filters, transform, feature\n",
        "from scipy import ndimage\n",
        "from math import sqrt, pi\n",
        "\n",
        "print(\"All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1 - Image Formation & Sampling\n",
        "print(\"=== TASK 1: Image Formation & Sampling ===\")\n",
        "\n",
        "# Load high-resolution image\n",
        "image_hr = data.camera()\n",
        "print(f\"Original image shape: {image_hr.shape}\")\n",
        "\n",
        "downsample_factors = [2, 4, 8]\n",
        "\n",
        "# Create subplots for visualization\n",
        "fig, axes = plt.subplots(2, len(downsample_factors) + 1, figsize=(16, 8))\n",
        "\n",
        "# Display original image\n",
        "axes[0, 0].imshow(image_hr, cmap='gray')\n",
        "axes[0, 0].set_title('Original High-Res Image')\n",
        "axes[0, 0].axis('off')\n",
        "\n",
        "axes[1, 0].imshow(image_hr, cmap='gray')\n",
        "axes[1, 0].set_title('Original High-Res Image')\n",
        "axes[1, 0].axis('off')\n",
        "\n",
        "for i, factor in enumerate(downsample_factors):\n",
        "    # Naive Downsampling (no prefiltering)\n",
        "    img_naive = image_hr[::factor, ::factor]\n",
        "\n",
        "    # Prefiltering with Gaussian Blur before downsampling\n",
        "    sigma = factor / 2  # Common heuristic\n",
        "    img_blurred = filters.gaussian(image_hr, sigma=sigma)\n",
        "    img_prefiltered = img_blurred[::factor, ::factor]\n",
        "\n",
        "    # Plotting\n",
        "    axes[0, i + 1].imshow(img_naive, cmap='gray')\n",
        "    axes[0, i + 1].set_title(f'Naive Downsample\\n(Factor {factor})')\n",
        "    axes[0, i + 1].axis('off')\n",
        "\n",
        "    axes[1, i + 1].imshow(img_prefiltered, cmap='gray')\n",
        "    axes[1, i + 1].set_title(f'Prefiltered Downsample\\n(Factor {factor}, σ={sigma})')\n",
        "    axes[1, i + 1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Frequency spectra analysis\n",
        "fig, axes = plt.subplots(2, len(downsample_factors) + 1, figsize=(16, 8))\n",
        "\n",
        "# Original image FFT\n",
        "fft_original = np.fft.fftshift(np.fft.fft2(image_hr))\n",
        "axes[0, 0].imshow(np.log1p(np.abs(fft_original)), cmap='viridis')\n",
        "axes[0, 0].set_title('Original FFT Spectrum')\n",
        "axes[0, 0].axis('off')\n",
        "\n",
        "axes[1, 0].imshow(np.log1p(np.abs(fft_original)), cmap='viridis')\n",
        "axes[1, 0].set_title('Original FFT Spectrum')\n",
        "axes[1, 0].axis('off')\n",
        "\n",
        "for i, factor in enumerate(downsample_factors):\n",
        "    # Naive downsampled FFT\n",
        "    img_naive = image_hr[::factor, ::factor]\n",
        "    fft_naive = np.fft.fftshift(np.fft.fft2(img_naive))\n",
        "\n",
        "    # Prefiltered FFT\n",
        "    sigma = factor / 2\n",
        "    img_blurred = filters.gaussian(image_hr, sigma=sigma)\n",
        "    img_prefiltered = img_blurred[::factor, ::factor]\n",
        "    fft_prefiltered = np.fft.fftshift(np.fft.fft2(img_prefiltered))\n",
        "\n",
        "    axes[0, i + 1].imshow(np.log1p(np.abs(fft_naive)), cmap='viridis')\n",
        "    axes[0, i + 1].set_title(f'Naive FFT (Factor {factor})')\n",
        "    axes[0, i + 1].axis('off')\n",
        "\n",
        "    axes[1, i + 1].imshow(np.log1p(np.abs(fft_prefiltered)), cmap='viridis')\n",
        "    axes[1, i + 1].set_title(f'Prefiltered FFT (Factor {factor})')\n",
        "    axes[1, i + 1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nExplanation of Prefiltering and Aliasing:\")\n",
        "print(\"\"\"\n",
        "Aliasing occurs when high-frequency components in the original signal fold back into lower frequencies\n",
        "due to insufficient sampling rate (violating the Nyquist criterion). Prefiltering applies a low-pass\n",
        "filter (Gaussian blur) before sampling to attenuate frequencies above the Nyquist limit (half the\n",
        "sampling rate). This prevents high frequencies from masquerading as lower frequencies, eliminating\n",
        "aliasing artifacts like moiré patterns and jagged edges. The Gaussian kernel's σ is typically chosen\n",
        "proportional to the scale factor to properly band-limit the signal.\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "Bm5EZlaAO607"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Task 2 - Linear Filters & Convolution (Manual Implementation)\n",
        "print(\"=== TASK 2: Linear Filters & Convolution ===\")\n",
        "\n",
        "def manual_conv2d(image, kernel):\n",
        "    \"\"\"Manual 2D convolution function with zero-padding.\"\"\"\n",
        "    k_h, k_w = kernel.shape\n",
        "    pad_h, pad_w = k_h // 2, k_w // 2\n",
        "\n",
        "    # Add zero-padding\n",
        "    image_padded = np.pad(image, ((pad_h, pad_h), (pad_w, pad_w)), mode='constant')\n",
        "    output = np.zeros_like(image, dtype=float)\n",
        "\n",
        "    # Perform convolution\n",
        "    for i in range(output.shape[0]):\n",
        "        for j in range(output.shape[1]):\n",
        "            region = image_padded[i:i + k_h, j:j + k_w]\n",
        "            output[i, j] = np.sum(region * kernel)\n",
        "    return output\n",
        "\n",
        "# Define Box filters\n",
        "box_3x3 = np.ones((3, 3)) / 9.0\n",
        "box_5x5 = np.ones((5, 5)) / 25.0\n",
        "\n",
        "# Create Gaussian kernel function\n",
        "def gaussian_kernel(size, sigma=1.0):\n",
        "    \"\"\"Create a 2D Gaussian kernel.\"\"\"\n",
        "    ax = np.linspace(-(size // 2), size // 2, size)\n",
        "    x, y = np.meshgrid(ax, ax)\n",
        "    kernel = np.exp(-(x**2 + y**2) / (2.0 * sigma**2))\n",
        "    return kernel / np.sum(kernel)\n",
        "\n",
        "# Generate Gaussian kernels\n",
        "gauss_3x3_s1 = gaussian_kernel(3, sigma=1.0)\n",
        "gauss_5x5_s2 = gaussian_kernel(5, sigma=2.0)\n",
        "\n",
        "print(\"Box 3x3 kernel:\")\n",
        "print(box_3x3)\n",
        "print(\"\\nGaussian 3x3 (σ=1) kernel:\")\n",
        "print(gauss_3x3_s1)"
      ],
      "metadata": {
        "id": "3HdFDM6aPFdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Task 2 - Filter Application and Comparison\n",
        "# Apply all filters manually and with OpenCV\n",
        "filters_to_test = [\n",
        "    ('Box 3x3', box_3x3),\n",
        "    ('Box 5x5', box_5x5),\n",
        "    ('Gaussian 3x3 σ=1', gauss_3x3_s1),\n",
        "    ('Gaussian 5x5 σ=2', gauss_5x5_s2)\n",
        "]\n",
        "\n",
        "# Convert image to float for processing\n",
        "image_float = image_hr.astype(float)\n",
        "\n",
        "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
        "\n",
        "for idx, (name, kernel) in enumerate(filters_to_test):\n",
        "    # Manual convolution\n",
        "    manual_result = manual_conv2d(image_float, kernel)\n",
        "\n",
        "    # OpenCV convolution\n",
        "    opencv_result = cv2.filter2D(image_float, -1, kernel)\n",
        "\n",
        "    # Calculate difference\n",
        "    diff = np.max(np.abs(manual_result - opencv_result))\n",
        "    print(f\"{name} - Max difference: {diff:.2e}\")\n",
        "\n",
        "    # Visualization\n",
        "    axes[0, idx].imshow(manual_result, cmap='gray')\n",
        "    axes[0, idx].set_title(f'Manual: {name}')\n",
        "    axes[0, idx].axis('off')\n",
        "\n",
        "    axes[1, idx].imshow(opencv_result, cmap='gray')\n",
        "    axes[1, idx].set_title(f'OpenCV: {name}')\n",
        "    axes[1, idx].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nTrade-off Analysis:\")\n",
        "print(\"\"\"\n",
        "Smoothing vs. Edge Sharpness Trade-off:\n",
        "- Larger kernels (5x5 vs 3x3) provide more aggressive noise reduction but cause greater edge blurring\n",
        "- Gaussian filters preserve edge quality better than box filters of equivalent size due to their\n",
        "  distance-weighted kernel, resulting in more natural transitions\n",
        "- Box 5x5: Strong smoothing but significant edge blurring\n",
        "- Gaussian 5x5 σ=2: Similar smoothing level but better edge preservation\n",
        "- Choice depends on application: noise reduction vs. feature preservation\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "5d4fRfcLOHid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Task 2 - Bonus: Convolution Commutativity\n",
        "print(\"=== BONUS: Convolution Commutativity ===\")\n",
        "\n",
        "# Test commutativity: A * B = B * A\n",
        "kernel_a = np.array([[1, 2], [3, 4]])\n",
        "kernel_b = np.array([[0, 1], [1, 0]])\n",
        "\n",
        "# Use a small test patch from the image\n",
        "test_patch = image_hr[100:102, 100:102].astype(float)\n",
        "print(\"Test patch:\")\n",
        "print(test_patch)\n",
        "\n",
        "# Apply A then B\n",
        "result_AB = manual_conv2d(manual_conv2d(test_patch, kernel_a), kernel_b)\n",
        "\n",
        "# Apply B then A\n",
        "result_BA = manual_conv2d(manual_conv2d(test_patch, kernel_b), kernel_a)\n",
        "\n",
        "print(f\"\\nResult of (A * B):\\n{result_AB}\")\n",
        "print(f\"\\nResult of (B * A):\\n{result_BA}\")\n",
        "print(f\"\\nMax absolute difference: {np.max(np.abs(result_AB - result_BA))}\")\n",
        "\n",
        "print(\"\\nCommutativity confirmed: A * B = B * A\")"
      ],
      "metadata": {
        "id": "X6fA_04BOLNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Task 3 - Edge Detection (Sobel and Prewitt Implementation)\n",
        "print(\"=== TASK 3: Edge Detection ===\")\n",
        "\n",
        "# Define Sobel and Prewitt kernels\n",
        "sobel_x = np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]])\n",
        "sobel_y = np.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]])\n",
        "\n",
        "prewitt_x = np.array([[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]])\n",
        "prewitt_y = np.array([[-1, -1, -1], [0, 0, 0], [1, 1, 1]])\n",
        "\n",
        "print(\"Sobel X kernel:\")\n",
        "print(sobel_x)\n",
        "print(\"\\nSobel Y kernel:\")\n",
        "print(sobel_y)\n",
        "print(\"\\nPrewitt X kernel:\")\n",
        "print(prewitt_x)\n",
        "print(\"\\nPrewitt Y kernel:\")\n",
        "print(prewitt_y)\n",
        "\n",
        "def compute_gradient_magnitude_orientation(img, kx, ky, name):\n",
        "    \"\"\"Compute gradient magnitude and orientation for given kernels.\"\"\"\n",
        "    Gx = cv2.filter2D(img.astype(float), -1, kx)\n",
        "    Gy = cv2.filter2D(img.astype(float), -1, ky)\n",
        "\n",
        "    magnitude = np.sqrt(Gx**2 + Gy**2)\n",
        "    orientation = np.arctan2(Gy, Gx)  # in radians\n",
        "\n",
        "    # Normalize magnitude for display\n",
        "    magnitude_display = (magnitude / magnitude.max() * 255).astype(np.uint8)\n",
        "\n",
        "    return Gx, Gy, magnitude, orientation, magnitude_display\n",
        "\n",
        "# Compute for Sobel\n",
        "Gx_sobel, Gy_sobel, mag_sobel, orient_sobel, mag_disp_sobel = compute_gradient_magnitude_orientation(\n",
        "    image_hr, sobel_x, sobel_y, \"Sobel\")\n",
        "\n",
        "# Compute for Prewitt\n",
        "Gx_prewitt, Gy_prewitt, mag_prewitt, orient_prewitt, mag_disp_prewitt = compute_gradient_magnitude_orientation(\n",
        "    image_hr, prewitt_x, prewitt_y, \"Prewitt\")"
      ],
      "metadata": {
        "id": "ZTROWAI1OOCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Task 3 - Edge Detection Visualization and Comparison\n",
        "# Visualize Sobel results\n",
        "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
        "\n",
        "# Sobel results\n",
        "axes[0, 0].imshow(Gx_sobel, cmap='seismic')\n",
        "axes[0, 0].set_title('Sobel - Gx (Horizontal)')\n",
        "axes[0, 0].axis('off')\n",
        "\n",
        "axes[0, 1].imshow(Gy_sobel, cmap='seismic')\n",
        "axes[0, 1].set_title('Sobel - Gy (Vertical)')\n",
        "axes[0, 1].axis('off')\n",
        "\n",
        "axes[0, 2].imshow(mag_disp_sobel, cmap='gray')\n",
        "axes[0, 2].set_title('Sobel - Gradient Magnitude')\n",
        "axes[0, 2].axis('off')\n",
        "\n",
        "axes[0, 3].imshow(orient_sobel, cmap='hsv')\n",
        "axes[0, 3].set_title('Sobel - Orientation')\n",
        "axes[0, 3].axis('off')\n",
        "\n",
        "# Prewitt results\n",
        "axes[1, 0].imshow(Gx_prewitt, cmap='seismic')\n",
        "axes[1, 0].set_title('Prewitt - Gx (Horizontal)')\n",
        "axes[1, 0].axis('off')\n",
        "\n",
        "axes[1, 1].imshow(Gy_prewitt, cmap='seismic')\n",
        "axes[1, 1].set_title('Prewitt - Gy (Vertical)')\n",
        "axes[1, 1].axis('off')\n",
        "\n",
        "axes[1, 2].imshow(mag_disp_prewitt, cmap='gray')\n",
        "axes[1, 2].set_title('Prewitt - Gradient Magnitude')\n",
        "axes[1, 2].axis('off')\n",
        "\n",
        "axes[1, 3].imshow(orient_prewitt, cmap='hsv')\n",
        "axes[1, 3].set_title('Prewitt - Orientation')\n",
        "axes[1, 3].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Compare with OpenCV built-in Sobel\n",
        "opencv_sobelx = cv2.Sobel(image_hr, cv2.CV_64F, 1, 0, ksize=3)\n",
        "opencv_sobely = cv2.Sobel(image_hr, cv2.CV_64F, 0, 1, ksize=3)\n",
        "opencv_mag = np.sqrt(opencv_sobelx**2 + opencv_sobely**2)\n",
        "\n",
        "# Normalize for comparison\n",
        "opencv_mag_display = (opencv_mag / opencv_mag.max() * 255).astype(np.uint8)\n",
        "\n",
        "print(\"Comparison with OpenCV Sobel:\")\n",
        "print(f\"Max difference in Gx: {np.max(np.abs(Gx_sobel - opencv_sobelx)):.2e}\")\n",
        "print(f\"Max difference in Gy: {np.max(np.abs(Gy_sobel - opencv_sobely)):.2e}\")\n",
        "\n",
        "print(\"\\nGradient Direction vs Edge Orientation Relationship:\")\n",
        "print(\"\"\"\n",
        "The gradient vector (Gx, Gy) points in the direction of the steepest intensity increase.\n",
        "Edge orientation is PERPENDICULAR to the gradient direction because edges represent\n",
        "boundaries between regions of different intensity.\n",
        "\n",
        "- Gradient Direction: θ = arctan2(Gy, Gx)\n",
        "- Edge Orientation: θ_edge = θ + 90°\n",
        "\n",
        "Thus, if the gradient points uphill across an edge, the edge itself runs perpendicular\n",
        "to this direction.\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "tPZPzxPHOQLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Task 3 - Bonus: Canny Edge Detector\n",
        "print(\"=== BONUS: Canny Edge Detector ===\")\n",
        "\n",
        "# Apply Canny with different thresholds\n",
        "edges_canny_low = cv2.Canny(image_hr, 50, 150)\n",
        "edges_canny_medium = cv2.Canny(image_hr, 100, 200)\n",
        "edges_canny_high = cv2.Canny(image_hr, 150, 250)\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "# Original image\n",
        "axes[0, 0].imshow(image_hr, cmap='gray')\n",
        "axes[0, 0].set_title('Original Image')\n",
        "axes[0, 0].axis('off')\n",
        "\n",
        "# Canny with different thresholds\n",
        "axes[0, 1].imshow(edges_canny_low, cmap='gray')\n",
        "axes[0, 1].set_title('Canny (Low: 50, 150)\\nMore edges, more noise')\n",
        "axes[0, 1].axis('off')\n",
        "\n",
        "axes[1, 0].imshow(edges_canny_medium, cmap='gray')\n",
        "axes[1, 0].set_title('Canny (Medium: 100, 200)\\nBalanced')\n",
        "axes[1, 0].axis('off')\n",
        "\n",
        "axes[1, 1].imshow(edges_canny_high, cmap='gray')\n",
        "axes[1, 1].set_title('Canny (High: 150, 250)\\nCleaner, fewer edges')\n",
        "axes[1, 1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Canny Detector Analysis:\")\n",
        "print(\"\"\"\n",
        "The Canny detector uses hysteresis thresholding with two thresholds:\n",
        "- Low threshold: Identifies potential edge pixels (including weak edges)\n",
        "- High threshold: Identifies strong, definite edge pixels\n",
        "\n",
        "Analysis:\n",
        "- Low thresholds (50, 150): Detect more edges including weak ones, but also more noise\n",
        "- Medium thresholds (100, 200): Good balance between edge completeness and noise reduction\n",
        "- High thresholds (150, 250): Only strongest edges remain, very clean but may miss details\n",
        "\n",
        "Hysteresis tracking connects weak edges to strong ones, providing better continuity\n",
        "than simple thresholding while maintaining noise robustness.\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "lsE3SFk_PbZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Task 4 - Geometric Transformations\n",
        "print(\"=== TASK 4: Geometric Transformations ===\")\n",
        "\n",
        "h, w = image_hr.shape\n",
        "print(f\"Image dimensions: {h} x {w}\")\n",
        "\n",
        "# 1. Translation (Shift 50px right, 30px down)\n",
        "M_translate = np.float32([[1, 0, 50], [0, 1, 30]])\n",
        "img_translated = cv2.warpAffine(image_hr, M_translate, (w, h))\n",
        "\n",
        "# 2. Rotation (30 degrees around image center)\n",
        "center = (w // 2, h // 2)\n",
        "angle = 30\n",
        "scale = 1.0\n",
        "M_rotate = cv2.getRotationMatrix2D(center, angle, scale)\n",
        "img_rotated = cv2.warpAffine(image_hr, M_rotate, (w, h))\n",
        "\n",
        "# 3. Scaling (Scale down to 80% around center)\n",
        "scale_factor = 0.8\n",
        "M_scale = cv2.getRotationMatrix2D(center, 0, scale_factor)\n",
        "img_scaled = cv2.warpAffine(image_hr, M_scale, (w, h))\n",
        "\n",
        "# 4. Perspective Warp\n",
        "src_pts = np.float32([[50, 50], [w-50, 50], [50, h-50], [w-50, h-50]])\n",
        "dst_pts = np.float32([[0, 0], [w, 100], [0, h], [w, h-100]])\n",
        "M_perspective = cv2.getPerspectiveTransform(src_pts, dst_pts)\n",
        "img_perspective = cv2.warpPerspective(image_hr, M_perspective, (w, h))\n",
        "\n",
        "print(\"Translation Matrix:\")\n",
        "print(M_translate)\n",
        "print(f\"\\nRotation Matrix (θ={angle}°):\")\n",
        "print(M_rotate)\n",
        "print(f\"\\nScaling Matrix (scale={scale_factor}):\")\n",
        "print(M_scale)\n",
        "print(\"\\nPerspective Transformation Matrix:\")\n",
        "print(M_perspective)"
      ],
      "metadata": {
        "id": "TSC3H2NePfXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Task 4 - Transformation Visualization and Analysis\n",
        "# Visualize all transformations\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "transformations = [\n",
        "    ('Translation\\n(50px right, 30px down)', img_translated),\n",
        "    ('Rotation\\n(30° around center)', img_rotated),\n",
        "    ('Scaling\\n(80% size)', img_scaled),\n",
        "    ('Perspective Warp', img_perspective)\n",
        "]\n",
        "\n",
        "for idx, (title, img) in enumerate(transformations):\n",
        "    ax = axes[idx // 2, idx % 2]\n",
        "    ax.imshow(img, cmap='gray')\n",
        "    ax.set_title(title)\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Transformation Matrix Analysis:\")\n",
        "print(\"\"\"\n",
        "AFFINE TRANSFORMATIONS (warpAffine):\n",
        "- Preserve parallelism and ratios of areas\n",
        "- Matrix form: M = [[a, b, c], [d, e, f]]\n",
        "  - c, f: Translation components (tx, ty)\n",
        "  - [[a,b],[d,e]]: Linear transformation (rotation, scaling, shearing)\n",
        "  - Rotation: a=e=cos(θ), b=-sin(θ), d=sin(θ)\n",
        "  - Scaling: a=sx, e=sy (diagonal elements)\n",
        "\n",
        "PERSPECTIVE TRANSFORMATION (warpPerspective):\n",
        "- Does NOT preserve parallelism (simulates 3D perspective)\n",
        "- 3x3 matrix with bottom row [v1, v2, v3] for projective effects\n",
        "- Creates vanishing points where parallel lines converge\n",
        "- More general than affine, can represent all affine + projective transforms\n",
        "\n",
        "Key Difference: Affine preserves parallel lines, perspective does not.\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "IUi-OClOPtqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Final Reflection and Summary\n",
        "print(\"=== REFLECTION & SUMMARY ===\")\n",
        "\n",
        "print(\"\"\"\n",
        "KEY INSIGHTS FROM THIS ASSIGNMENT:\n",
        "\n",
        "1. SAMPLING THEORY IS PRACTICAL: Task 1 made the Nyquist-Shannon theorem tangible.\n",
        "   Seeing severe aliasing in naïve downsampling versus clean prefiltered results\n",
        "   underscores that sampling requires careful frequency management.\n",
        "\n",
        "2. FILTERS INVOLVE TRADE-OFFS: Manual convolution implementation revealed the\n",
        "   computational nature of filtering. The Box vs Gaussian comparison showed that\n",
        "   stronger smoothing inherently compromises edge integrity - there's no free lunch.\n",
        "\n",
        "3. EDGES ARE FUNDAMENTAL FEATURES: Implementing Sobel/Prewitt demonstrated that\n",
        "   edge detection is more than kernel application. Understanding the perpendicular\n",
        "   relationship between gradient direction and edge orientation is crucial for\n",
        "   higher-level vision tasks.\n",
        "\n",
        "4. GEOMETRY HAS MATHEMATICAL ROOTS: The transformation exercises connected linear\n",
        "   algebra to visual outcomes. The distinction between affine (parallel-preserving)\n",
        "   and perspective (vanishing points) transformations provides the foundation for\n",
        "   image registration and 3D computer vision.\n",
        "\n",
        "REPRODUCIBILITY NOTE:\n",
        "All code is self-contained and uses standard computer vision libraries. The manual\n",
        "implementations (convolution, edge detection) verify our understanding of the\n",
        "underlying operations, while OpenCV comparisons ensure practical correctness.\n",
        "\n",
        "This module successfully bridges mathematical theory and practical implementation,\n",
        "providing the essential toolkit for advanced computer vision topics.\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "id": "ViUxFW7LPxej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cE-RKZRmSDZ9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}